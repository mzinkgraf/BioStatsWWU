---
title: "ANOVA"
author: "Ben Miner and Matthew Zinkgraf"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{ANOVA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteRender{all}
---


<h1>ANOVA</h1>

Analysis of variance (ANOVA) is a specific type of linear
model in which the predictor variable is a character and the response variable is numeric.  Linear models are a large group of statistical models that are very common in biological sciences.  The test statistic for linear models is the F-ratio. The distribution of the F-ratio (also called the F value) when the null hypothesis is true is described by the F distribution, and thus the P-value of a F-ratio is calculated from the F distribution.  Many other common statistical tests used by biologists are direct extensions of ANOVA (e.g., 2-factor ANOVA, and ANCOVA (analysis of covariance)).  

# Null and alternative hypotheses
You are only able to answer two-sided hypotheses with ANOVA--ANOVA is always a one-tailed test (the right, positive tail).  The null and alternative are always the following.

<ul>
  <li>Null hypothesis: true population means are equal, or $\mu_1 = \mu_2 \ldots \mu_k$
  <li>Alternative hypothesis: not the null hypothesis
</ul>

<div class="alert alert-warning">
Remember that only two groups need to differ to reject the null.   
</div>

# Data format
It is best to format your data with two columns, one for the predictor and one for response variable, with each row representing an independent replicate (or observation).  This is one of the common formats for a two-sample t-test.  Researchers typically enter their data into Excel and then import into R a csv or Excel file.  

# Flower morphology example
Let's analyze the data from the <samp>iris</samp> dataset that is included in R.  Because the data is already in R (it is loaded with the base packages), and we do not need to import these data.  This dataset includes 4 numeric metrics of flower morphology for three species.  An ANOVA is an appropriate test if we are interested in how the average of one of these metrics differs among the three species.  Let's first look at the structure of the dataset.

```{r}
str(iris)
```

Let's say that we want to test whether there are any differences in the mean length of flowers among the three species.  It is appropriate to analyze these data with an ANOVA because the predictor variable, species, is categorical, and the response variable, flower length, is numeric. 

<ul>
  <li>$H_0$: $\mu_{setosa}=\mu_{versicolor}=\mu_{virginica}$</li>
  <li>$H_A$: Not $H_0$</li>
</ul>

# By hand

## Test statistic
Below are the equations to calculate the sum of squares,
mean squares, and degrees of freedom for within and among the groups
for the above data. In the textbook, the authors use the term groups to represent the among group error, and the term error to describe the within group error.  Below, I use the terms among and within, instead of groups and error.  Both are common, so you need to practice thinking about both sets of terms.  Recall the following equations from lecture?  Hopefully you do because they are important.  

$$
{SS}_{among}=\Sigma_{j=1}^{a}\Sigma_{i=1}^{n} (\bar{Y}_{j}-\bar{Y})^2
$$

$$
{SS}_{within}=\Sigma_{j=1}^{a}\Sigma_{i=1}^{n} (Y_{j,i}-\bar{Y}_{j})^2
$$

$$
{df}_{among}=a-1
$$

$$
{df}_{within}=a\times(n-1) = N - a
$$

<div class="alert alert-warning">
An ANOVA should be balanced, and have the same number of replicates per treatment.  However, if the design is slightly unbalanced, you can still use an ANOVA, but should use the $N - a$ equation to determine the correct degrees of freedom for the within group error.   
</div>

$${MS}_{among}=\frac{{SS}_{among}}{{df}_{among}}$$

$${MS}_{within}=\frac{{SS}_{within}}{{df}_{within}}$$

where $a$ represents the number of groups (the book uses $k$ instead of $a$) and $n$ represents the number of replicates (or observations) in each treatment.  The total number of observations is often represented by $N$.  

<div class="alert alert-warning">
You will find different forms for the equations above.  For example, the book has different equations to calculate the mean squares.  All these forms are the same equation just rearranged.  I prefer the above equations because they represent the logic of the calculation.  However, they tend to inflate rounding errors if you use a calculator.  You are welcome to use which ever equation that make the most sense to you.   
</div>

You will want to calculate the means for each group and the grand mean, so you can use the above equations. If you recall, this is easily accomplished with the function <samp>tapply()</samp>.  

```{r}
(means <- tapply(iris$Petal.Length, iris$Species, mean))
#Or you can use the function with() so you don't have to write iris so often
(means <- with(iris, tapply(Petal.Width, Species, mean)))
```

Recall that hard brackets [ ] allow you to pull data from a vector or data.frame.  

```{r}
#A data.frame with only the rows for the setosa
setosa <- iris[iris$Species == "setosa", ]  #Notice I need two arguments (first for rows and second for columns)
head(setosa)
#A vector with the petal lengths only for setosa
(setosa_lengths <- iris$Petal.Length[iris$Species == "setosa"])
```

If you have time, you can modify these functions to calculate the sum of squares, mean squares, and F-ratio for these data.  I did not provide the code, so you can practice thinking about how to develop the code yourself, but make sure to ask if you are confused or get stuck!

Let's now look at the F distribution, so we can convert a F-ratio to a P value.  

## F distribution and P values
The distribution of F values when the null hypothesis is true is given by the F distribution.  We use the F distribution to convert a F value to a P value.  Like the Chi-squared distribution, we are only interested in the right-hand (or positive) tail of the distribution, and thus an ANOVA is a one-tailed test (which can only test two-sided hypotheses).  

Let's first explore the F distribution.  Plot the F distribution and
change the two parameters needed to describe its shape (the degrees
of freedom for the numerator (among group df) and the degrees of freedom for the
denominator (within group df)).  You have already done this in past labs for other distributions.  Here I show you how to specify the size of a new window to view our graph with the function <samp>windows()</samp> for PCs, and the function <samp>quartz()</samp> for Macs.  I set the width of the window to 4 inches and the height to 8 inches. Recall that <samp>par()</samp> allows the user to change the graphic parameters.  In this case, I use the argument <samp>mfrow</samp> to split the graphing window into two panels (2 rows and 1 column).  I also use the argument <samp>mar</samp> to set the figure margins in terms of number of lines.  You start with the bottom and work clockwise.  So, with the code below, I set the bottow margin to 4.5 lines, the left margin to 4.5 lines, the top margin to 0.5 lines, and the right margin to 0.5 lines.  

```{r, eval=-(1:3), fig.align='center', fig.height=8, fig.width=4}
#Open a window so two graphs fit one above the other
windows(width=4, height=8) #For PCs
qartz(width = 4, height = 8) #For Macs

#Split the window to plot two graphs and adjust the figure margins
par(mfrow=c(2, 1), mar=c(4,4,0,0)+0.5) 
#PDF of the F distribution with 2, 9 degrees of freedom
curve(df(x, 2, 9), 0, 10, ylim=c(0, 1), xlab = "F", ylab="Density")
#CDF of the F distribution with 2, 9 degrees of freedom
curve(pf(x, 2, 9), 0, 10, ylim=c(0, 1), xlab = "F", ylab="Prob")
```

Here is a more advanced example of how to visualize what happens to the F distribution when you change the degrees of freedom.  We already learned about almost all these functions in previous labs.  This might require a little time to think through the code (and might be something better done later), but you should understand it.  Remember that just copying what I have done will not help you in the future when you need to figure out what to do on your own.  

```{r, eval=-(1:2), fig.align='center',fig.height=8, fig.width=4}
windows(width = 4, height = 8)
quartz(width = 4, height = 8)

par(mfrow = c(2, 1), mar = c(4,4,0,0) + 0.5)

#Adjust the degrees of freedom for the within group error (i.e., denominator)
df.num <- 10
n <- 8
(df.den <- cumprod(1:n) )
(my.colors <- rainbow(n) )

curve(df(x, df.num, 1), 0, 6, ylab="Density", xlab = "F", ylim = c(0, 1))
for(i in 1:n) curve(df(x, df.num, df.den[i]), col=my.colors[i], add=TRUE)
legend(3, 1, df.den[1:n], lty = rep(1, n), col = my.colors, title = "within error df", bty = "n")

curve(pf(x, df.num, 1), 0, 6, ylab = "Probability", xlab = "F", ylim = c(0, 1))
for(i in 1:n) curve(pf(x, df.num, df.den[i]), col = my.colors[i], add = TRUE)

#Adjust the degrees of freedom for the among group error (i.e., numerator)
df.den <- 10
n <- 8
(df.num <- cumprod(1:n) )
(my.colors <- rainbow(n) )
curve(df(x, 1, df.den), 0, 6, ylab = "Density", xlab = "F", ylim = c(0, 1))
for(i in 1:n) curve(df(x, df.num[i], df.den), col = my.colors[i], add = TRUE)
legend(3, 1, df.num[1:n], lty = rep(1, n), col = my.colors, title = "among group df", bty = "n")

curve(pf(x, 1, df.den), 0, 6, ylab="Probability", xlab = "F", ylim = c(0, 1))
for(i in 1:n) curve(pf(x, df.num[i], df.den), col = my.colors[i], add = TRUE)
```

Now you can use the F distribution to calculate a P-value, or a critical value.  Let's assume that you caclulated an F-ratio of 5.6 (this is not the correct answer though, just made it up).  The among group degrees of freedom are 2 (3 groups minus 1), and within degrees of freedom are 147 (n is 50 and we have 3 groups, 3*(50 - 1)).  

```{r}
1 - pf(5.6, 2, 147)
#Or we can use the lower.tail argument, same as above
pf(5.6, 2, 147, lower.tail = FALSE)
```

We only need to worry about the right, positive tail when calculating the critical value for an ANOVA. So, we are after 5% of the area under the right tail, or 95% of the area under the left tail.  

```{r}
qf(0.95, 2, 147)
```

# <samp>lm()</samp> and <samp>aov()</samp>
Now let's get serious.  We first fit a linear model to the data and ask for R to produce the ANOVA table.  There are two functions that we can use to fit a linear model, <samp>lm()</samp> and <samp>aov()</samp>.  Both are commonly used, but <samp>aov()</samp> has a few benefits, like you can easily caculate run a Tukey-Kramer test after using <samp>aov()</samp>--<samp>aov()</samp> is a wrapper for <samp>lm()</samp>, which means that it actually uses <samp>lm()</samp> to calculate the linear model fit.  

Both functions require the same two arguments, a formula and the data.frame from which to pull the data. The formula (or model) tells R which variable is the response variable (first in the formula) and which variable is the predictor (second in the formula). The response and predictor variables are separated with <samp>\~</samp> (upper, left side of  your keyboard).  The <samp>\~</samp> sign is equivalent to the "=" sign for models in R. Formulas are heavily used in most statistical functions in R (and other programs).  

After we use <samp>lm()</samp> or <samp>aov()</samp>, we want to see the ANOVA table to determine whether to reject or fail to reject the null hypothesis.  We use either the function <samp>anova()</samp> or <samp>summary()</samp>, depending on which function we used to create the fit.  If we first fit with <samp>lm()</samp>, then we use <samp>anova()</samp>.  If we first fit with <samp>aov()</samp>, then we use <samp>summary()</samp>.  Here we go!  First with <samp>lm()</samp> and then <samp>aov()</samp>.  

```{r}
fit_lm <- lm(Petal.Length ~ Species, iris)
anova(fit_lm)
```

```{r}
fit_aov <- aov(Petal.Length ~ Species, iris)
summary(fit_aov)
```

You will notice they are produce the same output (an ANOVA table).  

You can check your calculations by hand against the values from the ANOVA table.  Do the values you calculated by hand match the results above?  They should (or be very close). 

And that is all there is to it!

# Displaying data
Just like for a two-sample t test, barplots (with error bars) and boxplots are the most common way to display data analyzed with an ANOVA.  Below I review how to make graphs with the base functions and ggplot2 functions.   

Making a boxplot is especially easy because R knows that a boxplot is an excellent graph when the predictor variable is categorical and the response is numeric.  We therefore just need to ask R to plot the data using the function <samp>plot()</samp>, which calls the <samp>boxplot()</samp> function when the predict is categorical and the response is numeric.  The two arguments that are needed for <samp>plot()</samp> (or the function <samp>boxplot()</samp>) are the same two arguments that we used for <samp>lm()</samp> and <samp>aov()</samp>, which are the formula and the data.frame. So, easy, which I love!!!

```{r}
plot(Sepal.Length ~ Species, iris)
```

We could also use <samp>boxplot()</samp>, which will give us the same graph.  

```{r}
plot(Sepal.Length ~ Species, iris)
```

To make a barplot with error bars, we use the function <samp>barplot()</samp> and <samp>arrows()</samp>.  The function <samp>barplot()</samp> requires a vector with the heights of the bars.  In our case that is the mean of each group.  We can name the barplot object, which will provide us with the x coorinates for the center of each bar in the graph.  The function <samp>arrows()</samp> requires the x and y coordinates for each error bar.  So, we need to also calculate whatever measure of spread we want to plot.  I will plot standard error in the example below.  

```{r}
iris_means <- with(iris, tapply(Sepal.Length, Species, mean))
iris_se <- with(iris, tapply(Sepal.Length, Species, function(x) sd(x)/sqrt(length(x))))

#You often need to adjust the y axis to include the tops of the error bars
x_vals <- barplot(iris_means,
    ylim = c(0, 8),
    ylab = "Sepal Length (units)"
)
arrows(x_vals, iris_means + iris_se, x_vals, iris_means - iris_se, angle = 90, code = 3)
#If I want to put a box around it
box()
```

Now let's make similar graphs using the package <samp>ggplot2</samp>.  If you use haven't practiced using this package, then please look at the <samp>ggplot2</samp> link under Graphing at the top of this page.  Making a boxplot is also very easily with <samp>ggplot2</samp>  

```{r}
library(ggplot2)
ggplot(iris, aes(x = Species, y = Sepal.Length)) +
  geom_boxplot()
```

Make a barplot with error bars is also easy in <samp>ggplot2</samp>.  The advantage of using <samp>ggplot2</samp> is that we don't need to calculate the means or measures of spread--we have the plotting function do it for us. 

```{r}
ggplot(iris, aes(x = Species, y = Sepal.Length)) +
  geom_bar(stat = "summary", fun.y = "mean") +
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.2) +
  ylab("Sepal length (units)")
```

You can also make the same plot, but using the <samp>stat_summary()</samp> function.  In this case, you give the geom as an argument.  Either using <samp>stat_summary()</samp> or <samp>geom_bar()</samp> and <samp>geom_errorbar()</samp> produce exactly the same graph, and I only provide both examples to illustrate that <samp>ggplot2</samp> is very flexible.  

```{r}
ggplot(iris, aes(x = Species, y = Sepal.Length)) +
  stat_summary(geom = "bar", fun.y = "mean") +
  stat_summary(geom = "errorbar", fun.data = "mean_se", width = 0.2) +
  ylab("Sepal length (units)")
```

If you install and load the <samp>Hmisc</samp> package, then you can use <samp>stat_summary</samp> to calculate the error bars that represent standard deviation, confidence intervals, and min-max (as well as others). You only have to switch the function for the argument <samp>fun.data</samp>.  Use <samp>mean_sdl</samp> to calculate and plot the standard deviation, and use <samp>mean_cl_normal</samp> to calculate and plot the 95% confidence interval. 

```{r}
#Requires the Hmisc package
#install.packages("Hmisc") #select no
#Plot standard deviation
ggplot(iris, aes(x = Species, y = Sepal.Length)) +
  stat_summary(geom = "bar", fun.y = "mean") +
  stat_summary(geom = "errorbar", fun.data = "mean_sdl", width = 0.2) +
  ylab("Sepal length (units)")

#Plot 95% confidence intervals
ggplot(iris, aes(x = Species, y = Sepal.Length)) +
  stat_summary(geom = "bar", fun.y = "mean") +
  stat_summary(geom = "errorbar", fun.data = "mean_cl_normal", width = 0.2) +
  ylab("Sepal length (units)")
```

# Assumptions
There are the following assumptions for an ANOVA.  

<ul>
  <li>Each replicate sample or observation is independent and a random sample from the population of interest.
  <li>All samples or observations are correctly categorized.
  <li>The residuals are normal distributed.
  <li>The variances are equal among groups.  
</ul>

The first assumption is only assessed by carefully thinking about the methods.  The second assumptions is almost always true unless the researcher was careless.  The third and fourth assumptions are assessed by looking at aspects of the data.  Below I demonstrate how to visually inspect these assumptions.  

Let's start with the assumption that the residuals are normally distributed.  The good news is that ANOVA is robust to this assumption.  In other words, we can still use an ANOVA when the residuals don't appear very normal.  You can easily check this assumption with a Q-Q plot.  A Q-Q plot is a graph in which the observed residuals are plotted against the predicted residuals is the data are normal.  So, the residuals are normal when the Q-Q plot is a straight line.  Hang on for just a second and I will show you how to easily create this plot.   
The assumption of equal variances among the groups is important, and ANOVA is not robust to this assumption.  This one is easy because you just need to compare the variances among the groups.  Of course they will not be perfectly equal, so we are really looking for cases in which the variance is one group is 3 or 4 times larger or smaller than another group.  

Again, R is going to really help us out because it will create the plot that we need to assess these two assumptions.  We just plot the fit from the <samp>lm()</samp> or <samp>aov()</samp> functions.  R will prompt you to hit ENTER to advance through the graphs.  Yes, it is really that easy!

We really care about the second and third graph.  The second graph is the Q-Q plot.  So, look for a straigt line (it will have a positive slope).  The third graph is a plot of the square root of the standardized residuals and fitted values, but what you care about is that the line is relatively straight with a slope of zero.  

```{r}
plot(fit_lm)
```

Both plots look good, and we can draw conclusion from our ANOVA with confidence. You will notice that the first group does have a smaller variance than the other two groups, but it is not so great to be of a concern.  

# Unplanned comparisons
If you have more than 2 groups and you reject the null hypothesis with an ANOVA, then you might want to know which groups differ from each other.  The Tukey-Kramer test is an excellent way to for unplanned comparisons of all group combinations. 

<div class="alert alert-warning">
Unlike ANOVA, the Tukey-Kramer test is not robust to the assumption of normality. So, make sure to assess this assumption with a Q-Q plot!  
</div>

We use the function <samp>TukeyHSD()</samp> to run a Tukey-Kramer test in R. However, it will only work if we use the aov fitted object (it doesn' work with the lm fitted object).  Again, it is that simple!

```{r}
TukeyHSD(fit_aov)
```

# More Complex Linear Models

You are now all set to analyze a wide range of more complex linear models.  You only need to modify the formula in the function <samp>lm()</samp> or <samp>aov()</samp>.  For example, let's say I have two categorical variables called <samp>f1</samp> and <samp>f2</samp> and a response variable called <samp>g</samp>, then I would use the formula, <samp>g ~ f1 * f2</samp> to run a two-factor ANOVA.  The "*" tells R that you want to test the main effects of <samp>f1</samp> and <samp>f2</samp>, and the interaction between <samp>f1</samp> and <samp>f2</samp>. 


<!-- <h1>Stuff from lab before I revised it</h1> -->
<!-- Just in case you what the info from lab on Friday, I have retained most of that info below.  If something is missing, then it is above in the revised lab.   -->

<!-- # ANOVA by hand -->
<!-- So, let's try an example.  Make the following -->
<!-- <samp>data.frame()</samp> and give it a name.  I will call mine <samp>data</samp>. -->

<!-- <table class="table table-striped" style="width:15%"> -->
<!-- <tr> -->
<!--   <th>f</th> -->
<!--   <th>g</th> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>a</td> -->
<!--   <td>6.9</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>b</td> -->
<!--   <td>8.3</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>a</td> -->
<!--   <td>7.3</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>b</td> -->
<!--   <td>6.3</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>a</td> -->
<!--   <td>11.2</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>b</td> -->
<!--   <td>8.1</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>a</td> -->
<!--   <td>13.6</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>b</td> -->
<!--   <td>5.7</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>a</td> -->
<!--   <td>8.0</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>b</td> -->
<!--   <td>13.8</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>a</td> -->
<!--   <td>3.3</td> -->
<!-- </tr> -->
<!-- <tr> -->
<!--   <td>b</td> -->
<!--   <td>12.8</td> -->
<!-- </tr> -->
<!-- </table> -->

<!-- Make a vector with the values from column f using the function <samp>c()</samp>, and name it <samp>f</samp>.  Now, make a vector with the values from column g1 using the function <samp>c()</samp>, and name it <samp>g1</samp>. You can then create your <samp>data.frame()</samp> with the following code.  All of this is review from the Data laboratory.   -->

<!-- ```{r} -->
<!-- f <- rep(c("a", "b"), 6) -->
<!-- g1 <- c(6.9, 8.3, 7.3, 6.3, 11.2, 8.1, 13.6, 5.7, 8, 13.8, 3.3, 12.8) -->
<!-- data<-data.frame(f, g1) -->
<!-- rm(f, g1) #Removes the objects f and g1 from memory -->
<!-- ``` -->

<!-- Make sure that <samp>f</samp> is a factor.  This is important because we will shortly learn how to run a linear model in R.  R will only know to perform an ANOVA if the predictor variable is a <samp>factor</samp> and the response variable is <samp>numeric</samp> or <samp>integer</samp>.   -->

<!-- You use the function <samp>sapply()</samp> to ask R about information for each column.  For example, <samp>sapply(data, class)</samp> will return the type of variable (i.e., the class) R has assigned each column, and <samp>class(data)</samp> returns the type of variable that R has assigned to <samp>data</samp>. The function <samp>summary()</samp> and <samp>str()</samp> are also useful for getting summary information about each column in a <samp>data.frame</samp>.  Try <samp>summary(data)</samp> and <samp>str(data)</samp>.   -->

<!-- ```{r} -->
<!-- class(data) -->
<!-- sapply(data, class) -->
<!-- summary(data) -->
<!-- str(data) -->
<!-- ``` -->

<!-- If you need to covert a variable to a factor in R, use the function <samp>factor()</samp>.  For example, let's just pretend like our <samp>f</samp> is not a factor.  We can use the code below to change it to a factor.   -->

<!-- ```{r} -->
<!-- data$f <- factor(data$f) #Be careful when you change an existing variable -->
<!-- ``` -->

<!-- Now let's use R as a calculator to compute the sum of squares, -->
<!-- mean squares, and degrees of freedom for within and among the groups -->
<!-- for the above data. Recall the following equations. -->

<!-- $$ -->
<!-- ss_{among}=\Sigma_{j=1}^{a}\Sigma_{i=1}^{n} (\bar{Y}_{j}-\bar{Y})^2 -->
<!-- $$ -->

<!-- $$ -->
<!-- ss_{within}=\Sigma_{j=1}^{a}\Sigma_{i=1}^{n} (Y_{j,i}-\bar{Y}_{j})^2 -->
<!-- $$ -->

<!-- $$ -->
<!-- ss_{total}=\Sigma_{j=1}^{a}\Sigma_{i=1}^{n} (Y_{j,i}-\bar{Y})^2 -->
<!-- $$ -->

<!-- $$ -->
<!-- df_{among}=a-1 -->
<!-- $$ -->

<!-- $$ -->
<!-- df_{within}=a\times(n-1) -->
<!-- $$ -->

<!-- $$ -->
<!-- df_{total}= a\times n-1 -->
<!-- $$ -->

<!-- where a is the number of groups, and n is the sample for each group.  -->

<!-- You can use the function <samp>tapply()</samp> to calculate the mean of <samp>g1</samp> for each level of <samp>f</samp> in your <samp>data.frame</samp>.  The function <samp>tapply</samp> take a numeric variable and then applies a function to each level of a factor.  Our numeric variable is <samp>g1</samp>, our factor is <samp>f</samp>, and the function is <samp>mean</samp>.  But of course you could change the function to calculate the standard deviation or some other value.   -->

<!-- ```{r} -->
<!-- #Both give same answer -->
<!-- (group.means <- tapply(data$g1, data$f, mean)) #Or -->
<!-- (group.means <- with(data, tapply(g1, f, mean)))  -->
<!-- ``` -->

<!-- Now convert the sum of squares to mean squares.  Think about what these numbers tell you about the relative variation associated with differences between the treatments and variation within the treatments.   -->

<!-- Calculate the F ratio, and then calculate the P-value for this F-ratio. Remember that, like the Chi-squared and G tests, the an ANOVA tests only two-sided hypotheses and is a one-tailed test (and you are interested in the right-hand, positive tail). -->

<!-- # ANOVA with R -->
<!-- There are several ways to perform an ANOVA in R.  However, all these functions use the function <samp>lm()</samp> as the base function to perform the calculations for the test.  So, we will just use the linear model function <samp>lm()</samp>.  This function -->
<!-- fits a linear model, and we can use several other functions to provide use with information about the fit of the linear model.  The function <samp>anova()</samp> displays all the ANOVA values you learned in class.  Type in the following. -->

<!-- ```{r} -->
<!-- lm.results <- lm(g1 ~ f, data=data) -->
<!-- anova(lm.results) -->
<!-- ``` -->



<!-- Now run a two-sample t-test -->
<!-- with 2-tails and compare the P-value with the ANOVA from -->
<!-- the linear model. Yes, a two-tailed two-sample t-test and a one-way -->
<!-- ANOVA give exactly the same result (that is to say the P-values of both tests are the same). -->

<!-- ANOVAs are useful because they allow you to test whether more -->
<!-- than two groups are different.  Add another factor to <samp>f</samp>, -->
<!-- say "c", and add some corresponding data to <samp>g1</samp>.  Remember you -->
<!-- can use the function <samp>edit()</samp> to quickly do this.  Don't forget -->
<!-- to name your edited data. -->

<!-- ```{r, eval=F} -->
<!-- new.data <- edit(data) -->
<!-- ``` -->

<!-- Now run an ANOVA on these data.  Think about carefully about the results.  Now change the data in a systematic -->
<!-- way, like making the means more different between the groups, but -->
<!-- keeping the variability the same, or vice versa.  What happens -->
<!-- to the F value?  What happens to the P-value? -->

<!-- There is one more thing I would like you to understand before we make graphs.  You can use the function <samp>plot()</samp> to graphically explore whether the data meet some of the assumptions of ANOVA. R will create diagnostic plots when you give the function <samp>plot()</samp> an object of class "lm", which is what is returned when you use the function <samp>lm()</samp>.  R will prompt you to click through four graphs.   -->

<!-- ```{r, fig.align='center'} -->
<!-- par(mfrow = c(2,2), mar = c(4,4,2,2)+0.5) #You don't need to run this line -->
<!-- plot(lm.results) -->
<!-- ``` -->

<!-- The first and third plot that are displayed (or the plots in the left-hand column) provide information about how the variance changes across the predictor variable (and is relatively unimportant for an ANOVA).  The second plot (or upper, right-hand plot) is called a Q-Q plot and provides information about whether the data are normally distributed.  If the data are normal, then the plot will show a straight line.  The last plot (or lower, right-hand plot) provides information about whether particular data points strongly influencing the results.   -->

<!-- You can also get a lot more information from the <samp>lm</samp> object.  Use the function <samp>names</samp> to see what information is stored in the object. -->

<!-- ```{r} -->
<!-- names(lm.results) -->
<!-- ``` -->

<!-- You can ask for this information using the object name, the dollar sign, and the name of the stored information.  For example, I can type the following to see the residuals. -->

<!-- ```{r} -->
<!-- lm.results$residuals -->
<!-- ``` -->

<!-- # Displaying Data -->

<!-- Like two-sample t-tests, barplots and boxplots are the two common ways that investigators display the data analyzed with an ANOVA.  Remember that R determines an appropriate graph based on the type of data you supply.  So, when you call the function <samp>plot()</samp> with a categorical predictor variable and a continuous response variable, R will produce a boxplot. You can use the same formula for <samp>plot()</samp> and for <samp>lm()</samp>. -->

<!-- ```{r, fig.align='center'} -->
<!-- plot(g1~f, data = data) -->
<!-- ``` -->

<!-- You can also use the functions <samp>barplot()</samp> and <samp>arrows()</samp>, which you have already learned.  Try and recall how to make a barplot with error bars.  Also, try and use the function <samp>ddply</samp> from the plyr package to summarize the data and calculate the means and standard error for each treatment (look at the plyr package lab for guidance).  You can then use these data to make barplots with error bars.   -->

<!-- You can also install and then load the gplots package, and use the function <samp>barplot2()</samp>, which will allow you to put error bars on your bargraph.   -->



